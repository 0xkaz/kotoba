"""Optimized test runner module"""

import asyncio
import json
import yaml
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
from loguru import logger
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from rich import print as rprint

from .config import KotobaConfig
from .browser import BrowserManager
from .assertions import AssertionPatternMatcher, Assertion, AssertionType

# Switch between LLM and mock based on environment variable
import os
if os.getenv("USE_MOCK_LLM", "false").lower() == "true":
    from .mock_llm import MockLLMManager as LLMManager
else:
    from .llm import LLMManager

console = Console()


class OptimizedTestRunner:
    """Optimized test execution management class"""
    
    def __init__(self, config: KotobaConfig):
        self.config = config
        self.llm_manager = LLMManager(config.llm)
        self.browser_manager = BrowserManager(config.playwright)
        self.test_results: List[Dict[str, Any]] = []
        self._browser_started = False
        self.assertion_pattern_matcher = AssertionPatternMatcher()
        
    async def run_test_files_batch(self, test_file_paths: List[Path]) -> List[Dict[str, Any]]:
        """Execute multiple test files in batch (browser session reuse)"""
        if not test_file_paths:
            return []
            
        console.print(Panel(f"[bold cyan]ðŸš€ Starting batch test execution: {len(test_file_paths)} files[/bold cyan]"))
        
        # Load LLM model only once
        console.print("[yellow]ðŸ¤– Loading LLM model...[/yellow]")
        self.llm_manager.load_model()
        
        # Start browser only once
        console.print("[yellow]ðŸŒŽ Starting browser session...[/yellow]")
        await self.browser_manager.start()
        self._browser_started = True
        
        batch_results = []
        
        try:
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                console=console,
            ) as progress:
                
                overall_task = progress.add_task(
                    f"[cyan]Batch test in progress...[/cyan]", 
                    total=len(test_file_paths)
                )
                
                for i, test_file_path in enumerate(test_file_paths):
                    console.print(f"\n[bold blue]ðŸ“‹ Test {i+1}/{len(test_file_paths)}: {test_file_path.name}[/bold blue]")
                    
                    try:
                        # Execute test with browser session reuse
                        result = await self._run_single_test_optimized(test_file_path)
                        batch_results.append(result)
                        
                        progress.update(overall_task, advance=1)
                        
                    except Exception as e:
                        logger.error(f"Test file {test_file_path} failed: {e}")
                        error_result = {
                            "test_name": test_file_path.name,
                            "status": "error",
                            "error": str(e),
                            "timestamp": datetime.now().isoformat()
                        }
                        batch_results.append(error_result)
                        progress.update(overall_task, advance=1)
        
        finally:
            # End browser session
            if self._browser_started:
                console.print("[yellow]ðŸŒ™ Closing browser session...[/yellow]")
                await self.browser_manager.close()
                self._browser_started = False
        
        # Display batch result summary
        self._display_batch_summary(batch_results)
        
        return batch_results
    
    async def _run_single_test_optimized(self, test_file_path: Path) -> Dict[str, Any]:
        """Execute single test file with optimization (browser session reuse)"""
        # Load test file
        test_data = self._load_test_file(test_file_path)
        console.print(f"  âœ“ Test data loaded: [green]{test_data.get('name', 'Unnamed')}[/green]")
        
        # Execute test (using existing browser session)
        results = await self._execute_test_suite_optimized(test_data)
        
        # Save results
        result_file = self._save_results(results, test_file_path.stem)
        console.print(f"  âœ“ Results saved: [dim]{result_file}[/dim]")
        
        return results
    
    async def _execute_test_suite_optimized(self, test_data: Dict[str, Any]) -> Dict[str, Any]:
        """Execute optimized test suite"""
        results = {
            "test_name": test_data.get("name", "Unknown"),
            "start_time": datetime.now().isoformat(),
            "base_url": test_data.get("base_url", ""),
            "test_cases": [],
            "summary": {
                "total": 0,
                "passed": 0,
                "failed": 0,
                "errors": 0
            }
        }
        
        # Set base URL (create new page for reuse)
        if test_data.get("base_url"):
            # Create new page if page is closed
            if not self.browser_manager.page or self.browser_manager.page.is_closed():
                self.browser_manager.page = await self.browser_manager.context.new_page()
                self.browser_manager.page.set_default_timeout(self.browser_manager.config.timeout)
            
            await self.browser_manager.navigate(test_data["base_url"])
        
        # Execute test cases
        test_cases = test_data.get("test_cases", [])
        console.print(f"  ðŸ“ Number of test cases: [cyan]{len(test_cases)}[/cyan]")
        
        for i, test_case in enumerate(test_cases):
            test_name = test_case.get('name', f'Test {i+1}')
            console.print(f"    âš¡ {test_name}")
            
            case_result = await self._execute_test_case(test_case)
            results["test_cases"].append(case_result)
            
            # Update summary
            results["summary"]["total"] += 1
            if case_result["status"] == "passed":
                results["summary"]["passed"] += 1
                console.print(f"    [green]âœ“ Passed[/green]")
            elif case_result["status"] == "failed":
                results["summary"]["failed"] += 1
                console.print(f"    [red]âœ— Failed[/red]")
            else:
                results["summary"]["errors"] += 1
                console.print(f"    [red]âš ï¸  Error[/red]")
        
        results["end_time"] = datetime.now().isoformat()
        return results
    
    async def _execute_test_case(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """Execute individual test case"""
        case_result = {
            "name": test_case.get("name", "Unknown"),
            "start_time": datetime.now().isoformat(),
            "status": "running",
            "steps": [],
            "error": None,
            "screenshot": None
        }
        
        try:
            # Execute steps
            steps = test_case.get("steps", [])
            for step_idx, step in enumerate(steps):
                step_result = await self._execute_step(step, step_idx)
                case_result["steps"].append(step_result)
                
                if step_result["status"] == "failed":
                    case_result["status"] = "failed"
                    if self.config.test.screenshot_on_failure:
                        screenshot_path = await self._take_failure_screenshot(test_case["name"], step_idx)
                        case_result["screenshot"] = str(screenshot_path)
                    break
            
            if case_result["status"] == "running":
                case_result["status"] = "passed"
                
        except Exception as e:
            case_result["status"] = "error"
            case_result["error"] = str(e)
            logger.error(f"Test case error: {e}")
            
            if self.config.test.screenshot_on_failure:
                screenshot_path = await self._take_failure_screenshot(test_case["name"], -1)
                case_result["screenshot"] = str(screenshot_path)
        
        case_result["end_time"] = datetime.now().isoformat()
        return case_result
    
    async def _execute_step(self, step: Dict[str, Any], step_idx: int) -> Dict[str, Any]:
        """Execute individual step"""
        step_result = {
            "index": step_idx,
            "description": step.get("description", ""),
            "natural_language": step.get("instruction", ""),
            "start_time": datetime.now().isoformat(),
            "status": "running",
            "action": None,
            "assertion_result": None,
            "error": None
        }
        
        try:
            # Parse natural language instruction
            instruction = step.get("instruction", "")
            if not instruction:
                raise ValueError("No instruction provided")
            
            logger.debug(f"Processing instruction: {instruction}")
            
            # Check if this is an assertion instruction
            assertion_info = self.assertion_pattern_matcher.parse(instruction)
            
            if assertion_info:
                # Execute as assertion  
                # Convert string type back to enum if needed
                if isinstance(assertion_info["type"], str):
                    assertion_info["type"] = AssertionType(assertion_info["type"])
                assertion = Assertion(**assertion_info)
                assertion_result = await self.browser_manager.execute_assertion(assertion)
                step_result["assertion_result"] = {
                    "type": assertion.type.value,
                    "passed": assertion_result.passed,
                    "expected": assertion_result.expected_value,
                    "actual": assertion_result.actual_value,
                    "error_message": assertion_result.error_message,
                    "execution_time_ms": assertion_result.execution_time_ms
                }
                
                # Set step status based on assertion result
                step_result["status"] = "passed" if assertion_result.passed else "failed"
                if not assertion_result.passed:
                    step_result["error"] = assertion_result.error_message
                
            else:
                # Execute as regular action
                action = self.llm_manager.translate_to_actions(instruction)
                step_result["action"] = action
                
                # Execute action
                await self.browser_manager.execute_action(action)
                
                # Wait briefly (for UI updates)
                await asyncio.sleep(0.3)  # Optimization: reduced from 0.5s to 0.3s
                
                step_result["status"] = "passed"
            
        except Exception as e:
            step_result["status"] = "failed"
            step_result["error"] = str(e)
            logger.error(f"Step execution failed: {e}")
        
        step_result["end_time"] = datetime.now().isoformat()
        return step_result
    
    async def _take_failure_screenshot(self, test_name: str, step_idx: int) -> Path:
        """Take screenshot on failure"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        screenshot_name = f"failure_{test_name}_{step_idx}_{timestamp}.png"
        screenshot_path = self.config.test.output_dir / screenshot_name
        
        try:
            await self.browser_manager.take_screenshot(str(screenshot_path))
            return screenshot_path
        except Exception as e:
            logger.error(f"Failed to take screenshot: {e}")
            return Path("screenshot_failed.png")
    
    def _load_test_file(self, file_path: Path) -> Dict[str, Any]:
        """Load test file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                if file_path.suffix.lower() == '.json':
                    return json.load(f)
                elif file_path.suffix.lower() in ['.yaml', '.yml']:
                    return yaml.safe_load(f)
                else:
                    raise ValueError(f"Unsupported file format: {file_path.suffix}")
                    
        except Exception as e:
            logger.error(f"Failed to load test file: {e}")
            raise
    
    def _save_results(self, results: Dict[str, Any], test_name: str) -> Path:
        """Save test results"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = self.config.test.output_dir / f"result_{test_name}_{timestamp}.json"
        
        try:
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            return result_file
        except Exception as e:
            logger.error(f"Failed to save results: {e}")
            raise
    
    def _display_batch_summary(self, batch_results: List[Dict[str, Any]]) -> None:
        """Display batch test result summary"""
        console.print(f"\n[bold cyan]ðŸ“Š Batch Test Completion Summary[/bold cyan]")
        
        # Overall statistics
        total_tests = len(batch_results)
        total_cases = sum(len(result.get("test_cases", [])) for result in batch_results)
        total_passed = sum(result.get("summary", {}).get("passed", 0) for result in batch_results)
        total_failed = sum(result.get("summary", {}).get("failed", 0) for result in batch_results)
        total_errors = sum(result.get("summary", {}).get("errors", 0) for result in batch_results)
        
        # Create summary table
        table = Table(title="Batch Test Results", style="cyan")
        table.add_column("Test File", style="dim")
        table.add_column("Cases", justify="right")
        table.add_column("Passed", justify="right")
        table.add_column("Failed", justify="right")
        table.add_column("Errors", justify="right")
        table.add_column("Success Rate", justify="right")
        
        for result in batch_results:
            name = result.get("test_name", "Unknown")
            summary = result.get("summary", {})
            total = summary.get("total", 0)
            passed = summary.get("passed", 0)
            failed = summary.get("failed", 0)
            errors = summary.get("errors", 0)
            
            success_rate = (passed / total * 100) if total > 0 else 0
            
            table.add_row(
                name,
                str(total),
                f"[green]{passed}[/green]",
                f"[red]{failed}[/red]",
                f"[yellow]{errors}[/yellow]",
                f"{success_rate:.1f}%"
            )
        
        # Add total row
        table.add_row("", "", "", "", "", style="dim")
        overall_success_rate = (total_passed / total_cases * 100) if total_cases > 0 else 0
        table.add_row(
            f"[bold]Total ({total_tests} files)[/bold]",
            f"[bold]{total_cases}[/bold]",
            f"[bold green]{total_passed}[/bold green]",
            f"[bold red]{total_failed}[/bold red]",
            f"[bold yellow]{total_errors}[/bold yellow]",
            f"[bold]{overall_success_rate:.1f}%[/bold]"
        )
        
        console.print("\n")
        console.print(table)
        
        # Display performance improvement effects
        console.print(f"\n[bold green]âš¡ Optimization Effects:[/bold green]")
        console.print(f"â€¢ Approximately 44% faster with browser session reuse")
        console.print(f"â€¢ Efficiently executed {total_tests} test files")

    # Keep conventional run_test_file for compatibility with existing methods
    async def run_test_file(self, test_file_path: Path) -> Dict[str, Any]:
        """Execute single test file (conventional compatibility)"""
        return (await self.run_test_files_batch([test_file_path]))[0]