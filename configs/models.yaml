# kotoba - Supported LLM Models

# Japanese-Specialized Models (Recommended)
japanese_models:
  # Recommended default - Well-balanced Japanese model
  rinna_gpt_neox_3_6b:
    model_name: "rinna/japanese-gpt-neox-3.6b"
    description: "3.6B parameter Japanese-specialized model, well-balanced"
    memory_usage: "~7GB VRAM"
    languages: ["ja"]
    
  # Lightweight Japanese model
  cyberagent_open_calm_3b:
    model_name: "cyberagent/open-calm-3b"
    description: "3B parameter Japanese-specialized model"
    memory_usage: "~6GB VRAM"
    languages: ["ja"]
    
  # Ultra-lightweight Japanese model
  lightOn_japanese_mamba_1_4b:
    model_name: "LightOn/japanese-mamba-1.4b"
    description: "1.4B parameter ultra-lightweight Japanese model"
    memory_usage: "~3GB VRAM"
    languages: ["ja"]

  # Plamo (PFN Japanese model)
  plamo_13b:
    model_name: "pfnet/plamo-13b"
    description: "PFN 13B parameter Japanese model, high performance"
    memory_usage: "~26GB VRAM"
    languages: ["ja", "en"]
    
  plamo_13b_instruct:
    model_name: "pfnet/plamo-13b-instruct"
    description: "PFN 13B instruction-tuned model"
    memory_usage: "~26GB VRAM"
    languages: ["ja", "en"]

# Multilingual Models
multilingual_models:
  # Microsoft Phi series
  phi_2:
    model_name: "microsoft/phi-2"
    description: "2.7B parameters, multilingual, lightweight and fast"
    memory_usage: "~5GB VRAM"
    languages: ["ja", "en", "zh", "ko", "fr", "de", "es"]
    
  phi_3_mini:
    model_name: "microsoft/Phi-3-mini-4k-instruct"
    description: "3.8B parameters, multilingual, instruction-focused"
    memory_usage: "~8GB VRAM"
    languages: ["ja", "en", "zh", "ko", "fr", "de", "es"]

  # Qwen series (Alibaba)
  qwen2_1_5b:
    model_name: "Qwen/Qwen2-1.5B"
    description: "1.5B parameters, multilingual, ultra-lightweight"
    memory_usage: "~3GB VRAM"
    languages: ["ja", "en", "zh", "ko", "fr", "de", "es", "ru"]
    
  qwen2_7b:
    model_name: "Qwen/Qwen2-7B"
    description: "7B parameters, multilingual, high performance"
    memory_usage: "~14GB VRAM"
    languages: ["ja", "en", "zh", "ko", "fr", "de", "es", "ru"]
    
  qwen2_instruct_1_5b:
    model_name: "Qwen/Qwen2-1.5B-Instruct"
    description: "1.5B instruction-tuned, lightweight and practical"
    memory_usage: "~3GB VRAM"
    languages: ["ja", "en", "zh", "ko", "fr", "de", "es", "ru"]

  # TinyLlama (Ultra-lightweight)
  tinyllama_1_1b:
    model_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    description: "1.1B parameters, ultra-lightweight, chat-focused"
    memory_usage: "~2GB VRAM"
    languages: ["en", "ja", "zh", "ko"]

# Recommended Configurations
recommended_configs:
  # Recommendations by Resource Constraints
  low_memory:  # Less than 8GB
    model: "Qwen/Qwen2-1.5B-Instruct"
    device: "cpu"
    
  medium_memory:  # 8-16GB
    model: "rinna/japanese-gpt-neox-3.6b"
    device: "auto"
    
  high_memory:  # 16GB or more
    model: "pfnet/plamo-13b-instruct"
    device: "auto"
    
  # Recommendations by Use Case
  japanese_focus:
    model: "rinna/japanese-gpt-neox-3.6b"
    description: "Specialized for Japanese language tasks"
    
  chinese_japanese:  # NEW: Based on benchmark results
    model: "Qwen/Qwen2-1.5B-Instruct"
    description: "Best for Chinese + Japanese applications (1.1s avg)"
    
  lightweight_fast:  # NEW: Based on benchmark results
    model: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    description: "Fastest execution with minimal resources (1.1s avg, 2GB)"
    
  multilingual:
    model: "Qwen/Qwen2-1.5B-Instruct"
    description: "Multi-language support including Chinese"
    
  performance:
    model: "pfnet/plamo-13b-instruct"
    description: "Highest accuracy for complex tasks"

# Note: Performance may vary depending on hardware, test complexity, and model configuration